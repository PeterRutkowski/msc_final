{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "name": "nn.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
<<<<<<< HEAD
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = 'comp12_int10_umap12rs69_dbscan40_uniform_k5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
=======
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "K8W4dsXhiCbw"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader"
      ],
      "execution_count": 1,
      "outputs": []
    },
>>>>>>> 1a9230e369daa683d0565cccdc6e8a3ec54f29e3
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pti__YYCkGwk",
        "outputId": "f8ebd712-17f1-42cd-eb93-39e8408934a3"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
<<<<<<< HEAD
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded = np.load('experiments/{}.npz'.format(experiment_name), allow_pickle=True)\n",
    "x_train = loaded['x_train']\n",
    "y_train = loaded['y_train']\n",
    "x_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "N, D_in, H1, H2, D_out = x_train.shape[0], x_train.shape[1], 2000, 1000, 10\n",
    "\n",
    "learning_rate = 0.001\n",
    "archstamp = '%dnodes_%flr' % (H1, learning_rate)\n",
    "\n",
    "x = torch.Tensor(x_train)\n",
    "y = np.squeeze(torch.LongTensor(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
=======
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k_P5b4KJn3Rk"
      },
      "source": [
        "def eval(experiment):\n",
        "  def evaluate_model(experiment, x, y):\n",
        "    model = torch.load('/content/drive/MyDrive/msc_experiments/{}_model.pt'.format(experiment), \n",
        "                      map_location=torch.device('cpu'))\n",
        "    x = np.load('/content/drive/MyDrive/msc_experiments/{}.npz'.format(experiment), allow_pickle=True)[x]\n",
        "    n_images = x.shape[0]\n",
        "    y = np.load('/content/drive/MyDrive/msc_experiments/{}.npz'.format(experiment), allow_pickle=True)[y]\n",
        "    x = torch.Tensor(x)\n",
        "    y = np.squeeze(torch.LongTensor(y))\n",
        "\n",
        "    model.eval()\n",
        "    outputs = model(x)\n",
        "    probals, predicted = torch.max(outputs, 1)\n",
        "\n",
        "    c = (predicted == y).squeeze()\n",
        "    correct = c.sum().item()\n",
        "\n",
        "    print('samples correct={} training accuracy={}'.format(correct, np.round(correct/n_images, 3)))\n",
        "  print('TRAIN')\n",
        "  evaluate_model(experiment, 'x_train', 'y_train')\n",
        "  print('TEST NONE')\n",
        "  evaluate_model(experiment, 'x_test_none', 'y_test')\n",
        "  print('TEST GAUSSIAN')\n",
        "  evaluate_model(experiment, 'x_test_gaussian', 'y_test')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P6WLNDfVr8FO"
      },
      "source": [
        "# training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cjv-Yodikp-_"
      },
      "source": [
        "experiment = 'comp12_int10_umap12rs69_dbscan35_uniform_k5'"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lbUV9E7CiCbz",
        "outputId": "d79106ad-3020-4c77-a4b2-0043e1066cef"
      },
      "source": [
        "path = '/content/drive/MyDrive/msc_experiments/{}.npz'.format(experiment)\n",
        "loaded = np.load(path, allow_pickle=True)\n",
        "x_train = loaded['x_train']\n",
        "y_train = loaded['y_train']\n",
        "x_train.shape, y_train.shape"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((10000, 2135), (10000,))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
>>>>>>> 1a9230e369daa683d0565cccdc6e8a3ec54f29e3
    {
      "cell_type": "code",
      "metadata": {
        "id": "t5w4PS2GiCbz"
      },
      "source": [
        "N, D_in, H1, H2, H3, D_out = x_train.shape[0], x_train.shape[1], 2000, 1000, 500, 10\n",
        "\n",
        "learning_rate = 0.001\n",
        "archstamp = '%dnodes_%flr' % (H1, learning_rate)\n",
        "\n",
        "x = torch.Tensor(x_train)\n",
        "y = np.squeeze(torch.LongTensor(y_train))"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qugY9B6AiCb0"
      },
      "source": [
        "model = torch.nn.Sequential(\n",
        "    torch.nn.Linear(D_in, H1),\n",
        "    torch.nn.Sigmoid(),\n",
        "    torch.nn.Dropout(p=0.5),\n",
        "    torch.nn.Linear(H1, H2),\n",
        "    torch.nn.Sigmoid(),\n",
        "    torch.nn.Dropout(p=0.5),\n",
        "    torch.nn.Linear(H2, H3),\n",
        "    torch.nn.ReLU(),\n",
        "    torch.nn.Linear(H3, D_out),\n",
        "    torch.nn.LogSoftmax(dim = 1),\n",
        ").cuda()\n",
        "\n",
        "loss_fn = torch.nn.NLLLoss()\n",
        "    \n",
        "losses = []\n",
        "\n",
        "batchs = 512\n",
        "    \n",
        "trainset = torch.utils.data.TensorDataset(x, y)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size = batchs,\n",
        "                                          shuffle=True, num_workers=1)\n",
        "\n",
        "#optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.5)\n",
        "\n",
        "batchesn = int(N/ batchs)\n",
        "epochs = 100"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F6Be4W3QiCb0",
        "outputId": "55519f9b-c138-428f-ca14-88acfae305ba"
      },
      "source": [
        "for epoch in range(epochs):\n",
        "    running_loss = 0.0\n",
        "    for i, datap in enumerate(trainloader, 0):\n",
        "        #print(i)\n",
        "        # get the inputs\n",
        "        inputs, labels = datap\n",
        "        inputs = inputs.to('cuda', non_blocking=True)\n",
        "        labels = labels.to('cuda', non_blocking=True)\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        outputs = model(inputs)\n",
        "\n",
        "        loss = loss_fn(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # print statistics\n",
        "        running_loss += loss.item()\n",
        "        if i % batchesn == batchesn-1:\n",
        "            print('[epoch {}] loss: {}'.format(epoch + 1, running_loss/batchesn))\n",
        "            running_loss = 0.0\n",
        "    scheduler.step()"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[epoch 1] loss: 2.3199971600582727\n",
            "[epoch 2] loss: 2.304309606552124\n",
            "[epoch 3] loss: 2.300133278495387\n",
            "[epoch 4] loss: 2.2249953244861804\n",
            "[epoch 5] loss: 2.0809848810497082\n",
            "[epoch 6] loss: 2.032510280609131\n",
            "[epoch 7] loss: 1.9565365565450568\n",
            "[epoch 8] loss: 1.8650492868925397\n",
            "[epoch 9] loss: 1.8118771063654047\n",
            "[epoch 10] loss: 1.7933795640343113\n",
            "[epoch 11] loss: 1.7663247208846242\n",
            "[epoch 12] loss: 1.7656056880950928\n",
            "[epoch 13] loss: 1.7537020131161338\n",
            "[epoch 14] loss: 1.7515329059801603\n",
            "[epoch 15] loss: 1.7438624469857467\n",
            "[epoch 16] loss: 1.7321690697419017\n",
            "[epoch 17] loss: 1.7270576200987164\n",
            "[epoch 18] loss: 1.7129966836226613\n",
            "[epoch 19] loss: 1.7172849303797673\n",
            "[epoch 20] loss: 1.7057985506559674\n",
            "[epoch 21] loss: 1.7092677354812622\n",
            "[epoch 22] loss: 1.6981556290074398\n",
            "[epoch 23] loss: 1.6983445882797241\n",
            "[epoch 24] loss: 1.6965904675031964\n",
            "[epoch 25] loss: 1.6836824542597721\n",
            "[epoch 26] loss: 1.6918412886167828\n",
            "[epoch 27] loss: 1.6883251290572316\n",
            "[epoch 28] loss: 1.688760048464725\n",
            "[epoch 29] loss: 1.680852959030553\n",
            "[epoch 30] loss: 1.6781493864561383\n",
            "[epoch 31] loss: 1.6788124159762734\n",
            "[epoch 32] loss: 1.6717618641100431\n",
            "[epoch 33] loss: 1.675865323919999\n",
            "[epoch 34] loss: 1.6703036207901805\n",
            "[epoch 35] loss: 1.6648780860398944\n",
            "[epoch 36] loss: 1.6661198390157599\n",
            "[epoch 37] loss: 1.6631628902334916\n",
            "[epoch 38] loss: 1.6590924012033563\n",
            "[epoch 39] loss: 1.6623110582954006\n",
            "[epoch 40] loss: 1.6604782342910767\n",
            "[epoch 41] loss: 1.6579798710973639\n",
            "[epoch 42] loss: 1.6646672800967568\n",
            "[epoch 43] loss: 1.6490070757113005\n",
            "[epoch 44] loss: 1.6449637977700484\n",
            "[epoch 45] loss: 1.6419761055394222\n",
            "[epoch 46] loss: 1.6516679776342291\n",
            "[epoch 47] loss: 1.648700281193382\n",
            "[epoch 48] loss: 1.6424414044932316\n",
            "[epoch 49] loss: 1.6441859195106907\n",
            "[epoch 50] loss: 1.6449190942864669\n",
            "[epoch 51] loss: 1.6319721372503984\n",
            "[epoch 52] loss: 1.6308714088640714\n",
            "[epoch 53] loss: 1.6314971321507503\n",
            "[epoch 54] loss: 1.6356513751180548\n",
            "[epoch 55] loss: 1.6343815452174137\n",
            "[epoch 56] loss: 1.62813265072672\n",
            "[epoch 57] loss: 1.6285693770960759\n",
            "[epoch 58] loss: 1.6254623312699168\n",
            "[epoch 59] loss: 1.6258364476655658\n",
            "[epoch 60] loss: 1.6249209391443353\n",
            "[epoch 61] loss: 1.6263895348498696\n",
            "[epoch 62] loss: 1.6265841032329358\n",
            "[epoch 63] loss: 1.624067419453671\n",
            "[epoch 64] loss: 1.6238501197413395\n",
            "[epoch 65] loss: 1.6142822378560115\n",
            "[epoch 66] loss: 1.6209496887106645\n",
            "[epoch 67] loss: 1.6190517701600726\n",
            "[epoch 68] loss: 1.6145315985930593\n",
            "[epoch 69] loss: 1.6186236268595646\n",
            "[epoch 70] loss: 1.6172484284953068\n",
            "[epoch 71] loss: 1.6153800111067922\n",
            "[epoch 72] loss: 1.6156106057919954\n",
            "[epoch 73] loss: 1.614524941695364\n",
            "[epoch 74] loss: 1.6116989600031\n",
            "[epoch 75] loss: 1.6148568768250315\n",
            "[epoch 76] loss: 1.6152297697569196\n",
            "[epoch 77] loss: 1.616422885342648\n",
            "[epoch 78] loss: 1.6125988081881875\n",
            "[epoch 79] loss: 1.6125746400732743\n",
            "[epoch 80] loss: 1.607804731318825\n",
            "[epoch 81] loss: 1.6060439348220825\n",
            "[epoch 82] loss: 1.6141944182546515\n",
            "[epoch 83] loss: 1.6111294909527427\n",
            "[epoch 84] loss: 1.6049360225075169\n",
            "[epoch 85] loss: 1.6080978920585232\n",
            "[epoch 86] loss: 1.609871362384997\n",
            "[epoch 87] loss: 1.609784904279207\n",
            "[epoch 88] loss: 1.605952495022824\n",
            "[epoch 89] loss: 1.5986441499308537\n",
            "[epoch 90] loss: 1.6048712730407715\n",
            "[epoch 91] loss: 1.6008205351076628\n",
            "[epoch 92] loss: 1.6060334255820827\n",
            "[epoch 93] loss: 1.598233888023778\n",
            "[epoch 94] loss: 1.6024721045243113\n",
            "[epoch 95] loss: 1.5963334723522788\n",
            "[epoch 96] loss: 1.5919481390400936\n",
            "[epoch 97] loss: 1.5973765222649825\n",
            "[epoch 98] loss: 1.6011986857966374\n",
            "[epoch 99] loss: 1.5952160044720298\n",
            "[epoch 100] loss: 1.5937306818209196\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sZKT9KH9iCb1"
      },
      "source": [
        "np.save('/content/drive/MyDrive/msc_experiments/{}_losses.npz'.format(experiment), np.array(losses))\n",
        "model_filen = experiment + '.pt' \n",
        "\n",
        "torch.save(model, '/content/drive/MyDrive/msc_experiments/{}_model.pt'.format(experiment))"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SbmIud-msDr-"
      },
      "source": [
        "# testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cJmK5AIWlPin",
        "outputId": "d8a492f0-37f5-485d-a4e5-836a92082cea"
      },
      "source": [
        "eval(experiment)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TRAIN\n",
            "samples correct=4459 training accuracy=0.446\n",
            "TEST NONE\n",
            "samples correct=1155 training accuracy=0.385\n",
            "TEST GAUSSIAN\n",
            "samples correct=1126 training accuracy=0.375\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}